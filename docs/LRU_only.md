LRU --- Leaset Recently Used

## LRU 作为做常见的缓存策略，也就是淘汰掉最近最久未被访问的数据

**适用场景：**
-- 存在明显 “热点数据”的情况
-- 访问频率比较稳定，较少短期突变的场景

**LRU 设计思路：**
-- double linked list， 维护capacity + 数据
-- 把最新的数据放到最前面，如果capacity超了，pop 最后面的node
-- 通过维护时间顺序，尽量提高get的命中率

## Interface && implementation

### 顶层接口： `CachePolicy.h`
这个项目是想写所有的缓存策略的合集，那么这里就设计了一个顶层的接口
用于实现所有的缓存算法的
这个接口就包括定义了pure virtual function

```cpp
template<typename Key, typename Value>
class CachePolicy {
public:
    virtual void put(const Key&, const Value&) = 0;
    virtual bool get(const Key&, Value&) = 0;
    virtual Value get(const Key&) = 0;
    virtual ~CachePolicy() = default;
};
```

## LRU接口和实现
这里的LRU的头文件
    声明了LruNode节点类：双向链表结构，记录 Key/Value
    声明了KLruCache节点类：继承接口，实现Lru逻辑

LRU的实现文件
- std::unordered_map<Key, NodePtr> O(1) access
- double linked list 维护访问的顺序，用dummyHead/dummyTail作为头尾
- 用了 std::weak_ptr && std::shared_ptr
    next_  : shared_ptr  → 拥有后继节点的所有权
    prev_  : weak_ptr    → 观察前驱节点（不增加引用数，避免循环引用）

同时为了确保多线程的操作下的安全性，在put/get/remove的情况下，加入了mutex
确保数据的安全性和统一性


## 同时实现了LruCache 和 HashLruCache
- KLruCache
    
    **只有访问 ≥ K 次的数据才允许进入主缓存，防止冷数据污染缓存。**

    是对于LRU的改进，用了两个队列来实现，一个是缓存队列，一个是数据访问历史队列。当访问一个数据的时候，现在访问历史队列中累加访问次数，当历史访问记录超过k次之后，才将数据缓存到缓存队列，这样可以避免缓存队列被污染。
    · 缓存污染：访问很少的数据在服务完访问请求之后还留在缓存中，造成了缓存空间的浪费
    要注意的点就是，k值越大，缓存的命中率越高，但是同时也会让缓存难以被淘汰。

- HashLruCache · 轻量级LRU
    把一个大缓存切成多个小片段，每个片段维护一个独立的 LRU，这样多个线程访问不同 key 时不会互相锁住。

**适应于高并发场景**
对于大量同步等待操作导致耗时增加的情况，解决方案就是尽量的减少临界区。引入hash，从而降低查询耗时。将缓存数据分散到N个LruCache上面，查询的时候要按照相同的哈希算法，先获取数据可能存在的分片，然后再去对应的分片上查询数据。这样可以增加lru的读写操作的并行度，减少同步等待的耗时。


## 测试的思路

测试的目的是为了验证LRU的基本功能
测试命中率在不同访问模式下的表现
对比不同的变量(capacity， 访问模式)对结果的影响

### 场景设计

1. **热点访问场景 (Hot Access)**
   **目的**：模拟大部分访问集中在少数 key 上，比如热门网页或商品。

    | 测试     | 参数              | 说明           | 预期结果          |
    |--------|--------------------|----------------|----------------|
    | Test 1 | CAP=20, HOT=20     | 基线测试        | LRU 命中率中等  |
    | Test 2 | CAP=40             | 增大缓存容量    | 命中率明显提升        |
    | Test 3 | HOT=10             | 热点更集中      | 命中率进一步上升      |
    | Test 4 | PUT=60%            | 写操作比例增加  | 可能会影响命中率       |
   

2. **LRU-K 对比 LRU**
   **目的**：验证当热点访问频繁时，LRU-K 是否更能识别并缓存热点数据。
    | 测试   | 参数  | 说明             | 预期结果           |
    |--------|------|------------------|----------------|
    | Test 5 | k=2  | LRU-K vs LRU     | LRU-K 命中率略高     |
    | Test 6 | k=3  | 更严格的提升规则   | 可能因晋升门槛高反而效果不如 LRU |

3. **循环扫描场景 (Loop Scan)**
    **目的**：模拟数据循环访问场景，常用于顺序读块的模式，观察缓存策略的应对能力。

    | 测试     | 参数             | 说明     | 预期结果                |
    |--------|-------------------|--------|---------------------|
    | Test 7 | CAP=20, LOOP=500  | LRU    | 命中率非常低（缓存频繁失效）   |
    | Test 8 | k=2               | LRU-K  | 提升有限或无明显效果         |

4. **冷数据场景 (Cold Data)**

    **目的**：模拟每次访问的 key 都是新数据，对任何缓存策略都是极端不利场景。

    | 测试      | 参数                | 说明     | 预期结果            |
    |---------|----------------------|--------|-----------------|
    | Test 9  | CAP=20, RANGE=10000  | LRU    | 命中率几乎为 0        |
    | Test 10 | k=2                  | LRU-K  | 与 LRU 表现基本一致    |

5. **Hash-LRU 对比测试**

    **目的**：验证 Hash 分片缓存策略在多线程或分布式访问下是否带来提升。

    | 测试     | 参数       | 说明                        | 预期结果               |
    |---------|-------------|---------------------------|--------------------|
    | Test 11 | slice=1     | Hash-LRU 基线配置             | 与普通 LRU 相近          |
    | Test 12 | slice=4     | 开启多分片                    | 命中率略有提升（访问并发解耦）  |
    | Test 13 | LOOP 场景    | 分片在顺序扫描下不一定占优     | 可能与普通 LRU 相近       |
    | Test 14 | Cold Data   | 分片策略对冷数据无明显优化效果  | 命中率仍非常低           |

## Test Result

=== Test 1: Baseline (CAPACITY=20, HOT_KEYS=20) ===
GETs: 70146, Hits: 34511, Hit Rate: 49.20%
// baseline 命中率中等，表示热点刚好覆盖满缓存，频繁写入与替换会让缓存不稳定

=== Test 2: Increase Capacity (CAPACITY=40) ===
GETs: 69905, Hits: 49012, Hit Rate: 70.11%
// 容量翻倍，有更多空间存热点 key，命中率显著提升

=== Test 3: Reduce Hot Keys (HOT_KEYS=10) ===
GETs: 70218, Hits: 48889, Hit Rate: 69.62%
// 热点集中性更强，缓存能更快收敛热点 key，命中率也接近 Test 2

=== Test 4: High PUT rate (PUT=60%) ===
GETs: 39931, Hits: 18036, Hit Rate: 45.17%
// 写多导致缓存数据更换更频繁，命中率下降，符合预期

-------------------------------------------------
热点访问场景整体命中率变动与参数成正相关，
说明 LRU 能比较敏感地响应热点 key 的集中度和容量变化。
-------------------------------------------------

=== Test 5: LRU-K vs LRU (k=2) ===
GETs: 70221, Hits: 48595, Hit Rate: 69.20%
// 命中率接近 Test 2 和 3，略低于 LRU，因为需要达到两次访问才晋升为缓存内容

=== Test 6: LRU-K (k=3) ===
GETs: 70264, Hits: 48957, Hit Rate: 69.68%
// 略高于 Test 5，表示热 key 被稳定晋升，但也说明 Test 5 可能晋升略慢，Test 6 参数反而合理

-------------------------------------------------
LRU-K 整体命中率与普通 LRU 相差不大，在热点足够集中的情况下效果不错，尤其当热点 key 能快速晋升。Test 6 表现比想象中好，说明热点 key 访问非常频繁。
-------------------------------------------------


=== Test 7: Loop Scan LRU ===
GETs: 79992, Hits: 0, Hit Rate: 0.00%
// 每次访问 key 都被替换，基本没有复用空间

=== Test 8: Loop Scan LRU-K (k=2) ===
GETs: 79971, Hits: 0, Hit Rate: 0.00%
// 无法达到晋升次数，必然全 miss

-------------------------------------------------
完美符合预期，循环访问和 FIFO 特性导致缓存彻底失效。这类访问模式就是 LRU 系策略的弱点。
-------------------------------------------------


=== Test 9: All Cold Data LRU ===
GETs: 50000, Hits: 50000, Hit Rate: 100.00%
// 正常应该全 miss，但在 put 后立刻 get，所以每个值都被命中。可以算成“写后立刻读”而非真实 cold data

=== Test 10: All Cold Data LRU-K (k=2) ===
GETs: 50000, Hits: 232, Hit Rate: 0.46%
// 由于不能达到 K 次访问，基本都 miss，符合 cold data 的效果

-------------------------------------------------
Test 9 的“命中率100%”是因为测试逻辑不是 cold scan，而是 write-followed-by-read。可以接受，但文档里要注明：“此为 put 后立即 get 场景，不是纯 cold miss”。
-------------------------------------------------

=== Test 11: Hash LRU (default slice) ===
GETs: 70208, Hits: 43176, Hit Rate: 61.50%
// 表现优于 baseline LRU（Test 1），但略逊于 LRU-K（Test 6）

=== Test 12: Hash LRU (4 slices) ===
GETs: 70212, Hits: 34233, Hit Rate: 48.76%
// 多分片本应提升并发访问能力，但命中率下降，可能 key 分布不均或每片容量太小

=== Test 13: Loop Scan Hash LRU ===
GETs: 80223, Hits: 150, Hit Rate: 0.19%
// 微弱提升，分片略微减缓了 key 被踢出，但仍然是坏场景

=== Test 14: All Cold Data Hash LRU ===
GETs: 50000, Hits: 50000, Hit Rate: 100.00%
// 说明 put 后立即 get，导致命中率人为偏高

-------------------------------------------------
Hash LRU 的 slice 设置要根据容量配合，不然小 slice 会导致局部命中率下降。
Test 12 的表现说明 多分片不等于一定更优，具体还需合理设置 key hash 分布和 slice 容量。
Cold Data 测试中 put 后立即 get 的逻辑要特别注意，和实际场景偏差大。
-------------------------------------------------

### 结果分析

LRU：
    优点：实现简单，命中率稳定。
    缺点：易受冷数据、顺序扫描影响，缺乏“过滤能力”。

LRU-K：
    优点：能有效过滤掉一次性访问，提高热点命中率。
    缺点：初始化阶段“冷启动”，实现相对复杂，不适合对延迟敏感的业务。

Hash-LRU：
    优点：天然适用于多线程/多租户高并发环境；防止集中热点冲突。
    缺点：命中率依赖 key 分布；分片过多会导致单组容量太小、效率下降。

### 编译方式（使用 CMake）

```bash
cmake -Bbuild
cmake --build build
./build/test_lru_only
